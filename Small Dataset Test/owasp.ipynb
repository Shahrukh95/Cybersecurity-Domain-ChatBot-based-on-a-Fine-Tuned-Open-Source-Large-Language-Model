{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the CUDA_VISIBLE_DEVICES environment variable\n",
    "# 40 GB\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"MIG-a4a538a2-a199-58d8-9e56-ed86db02edf8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sahsan/PythonProjects/pytorch/Falcon7B-manual/manual_py_3_8_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, GenerationConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95994ecf1c374020a02b79adbcd138dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load the dataset from the formatted CSV file\n",
    "# data = load_dataset('csv', data_files='formatted-data.csv')\n",
    "# # dataset = load_dataset('csv', data_files='owasp-formatted.csv')\n",
    "\n",
    "# # Accessing the dataset\n",
    "# # print(repr(data[0]['text']))  # Print the first row of the dataset\n",
    "# # print(repr(data[\"train\"][0]['text']))\n",
    "# print(repr(data[\"train\"][5]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<HUMAN>: Are there cures for mental health problems?\\n<ASSISTANT>: When it comes to mental health problems, it\\'s important to clarify that the term \"cure\" might not be the most accurate way to describe the situation.\\n\\nMental health issues can be complex and varied, and there isn\\'t always a straightforward \"cure\" like there might be for some physical ailments. However, many mental health conditions can be effectively managed, treated, and even overcome with the right support, interventions, and coping strategies.\\n\\nTreatment options often include therapy, counseling, medication, lifestyle changes, and self-help techniques. The goal is to improve a person\\'s overall well-being and ability to cope with challenges rather than just eliminating the problem entirely.\\n\\nIt\\'s essential to seek professional help if you or someone you know is struggling with mental health concerns. A mental health professional can provide personalized guidance and support tailored to individual needs. Remember, it\\'s okay to ask for help, and with the right resources, recovery and improvement are possible.'\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"ZahrizhalAli/mental_health_conversational_dataset\")\n",
    "data\n",
    "\n",
    "# Take a Glance on how the data looks like \n",
    "print(repr(data[\"train\"][10]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114662ff643c416f85f2f09ff82852f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            # load model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n",
    "    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config, # Use bitsandbytes config\n",
    "    device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n",
    "    trust_remote_code=True, # Set trust_remote_code=True to use falcon-7b model with custom code\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) # Set trust_remote_code=True\n",
    "tokenizer.pad_token = tokenizer.eos_token # Setting pad_token same as eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT (Parameter Efficient Fine Tuning) and QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_alpha = 32 # scaling factor for the weight matrices\n",
    "lora_dropout = 0.05 # dropout probability of the LoRA layers\n",
    "lora_rank = 32 # dimension of the low-rank matrices\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_rank,\n",
    "    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[         # Setting names of modules in falcon-7b model that we want to apply LoRA to\n",
    "        \"query_key_value\",\n",
    "        \"dense\",\n",
    "        \"dense_h_to_4h\",\n",
    "        \"dense_4h_to_h\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./models/falcon-7b-smaller-cyber-data\"\n",
    "per_device_train_batch_size = 2 # reduce batch size by 2x if out-of-memory error\n",
    "gradient_accumulation_steps = 2  # increase gradient accumulation steps by 2x if batch size is reduced\n",
    "optim = \"paged_adamw_32bit\" # activates the paging for better memory management\n",
    "save_strategy=\"steps\" # checkpoint save strategy to adopt during training\n",
    "save_steps = 10 # number of updates steps before two checkpoint saves\n",
    "logging_steps = 10  # number of update steps between two logs if logging_strategy=\"steps\"\n",
    "learning_rate = 2e-4  # learning rate for AdamW optimizer\n",
    "max_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\n",
    "max_steps = 320        # training will happen for 320 steps\n",
    "warmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\n",
    "lr_scheduler_type = \"cosine\"  # learning rate scheduler\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=False,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    push_to_hub=True,\n",
    "    tf32=False,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=data['train'],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 38:13, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.440600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.381500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.222100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.835500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.764300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.788700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.479700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.496900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.497900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.252900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.092600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.089300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=0.6114265356212855, metrics={'train_runtime': 2308.0423, 'train_samples_per_second': 0.555, 'train_steps_per_second': 0.139, 'total_flos': 1.1442388164380928e+16, 'train_loss': 0.6114265356212855, 'epoch': 7.27})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/shahrukh95/falcon-7b-smaller-cyber-data/tree/main/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorboard import notebook\n",
    "# log_dir = \"cybersecurity-Llama-2-7b-chat-hf/runs\"\n",
    "# notebook.start(\"--logdir {} --port 4001\".format(log_dir))\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir {log_dir} --port 4000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51394028995344a5aafea985a794cb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading original model\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c633bfaf7c6424e8cfaf895b5814f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/534 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5573c7286214efa9d7f410ce0e35b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c492c6e5024e4d29a1336ae12bba9b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading PEFT model\n",
    "PEFT_MODEL = \"shahrukh95/falcon-7b-smaller-cyber-data\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "peft_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)\n",
    "\n",
    "peft_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "peft_tokenizer.pad_token = peft_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate responses from both original model and PEFT model and compare their answers.\n",
    "def generate_answer(query):\n",
    "  system_prompt = \"\"\"Answer the following question truthfully.\n",
    "  If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n",
    "  If the question is too complex, respond 'Kindly, consult a psychiatrist for further queries.'.\"\"\"\n",
    "\n",
    "  user_prompt = f\"\"\": {query}\n",
    "  : \"\"\"\n",
    "\n",
    "  final_prompt = system_prompt + \"\\n\" + user_prompt\n",
    "\n",
    "  device = \"cuda:0\"\n",
    "  dashline = \"-\".join(\"\" for i in range(50))\n",
    "\n",
    "  encoding = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "  outputs = model.generate(input_ids=encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=256, pad_token_id = tokenizer.eos_token_id, \\\n",
    "                                                                                                                     eos_token_id = tokenizer.eos_token_id, attention_mask = encoding.attention_mask, \\\n",
    "                                                                                                                     temperature=0.4, top_p=0.6, repetition_penalty=1.3, num_return_sequences=1,))\n",
    "  text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "  print(dashline)\n",
    "  print(f'ORIGINAL MODEL RESPONSE:\\n{text_output}')\n",
    "  print(dashline)\n",
    "\n",
    "  peft_encoding = peft_tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "  peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=256, pad_token_id = peft_tokenizer.eos_token_id, \\\n",
    "                                                                                                                     eos_token_id = peft_tokenizer.eos_token_id, attention_mask = peft_encoding.attention_mask, \\\n",
    "                                                                                                                     temperature=0.4, top_p=0.6, repetition_penalty=1.3, num_return_sequences=1,))\n",
    "  peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "  print(f'PEFT MODEL RESPONSE:\\n{peft_text_output}')\n",
    "  print(dashline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Answer the following question truthfully.\n",
      "  If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n",
      "  If the question is too complex, respond 'Kindly, consult a psychiatrist for further queries.'.\n",
      ": What to do if you have mental illnes?\n",
      "  : 1. Consult a psychiatrist.\n",
      "  : 2. Consult a psychologist.\n",
      "  : 3. Consult a psychoanalyst.\n",
      "  : 4. Consult a psychotherapist.\n",
      "  : 5. Consult a psychotherapist.\n",
      ": What to do if you have mental illnes?\n",
      "  : 1. Consult a psychiatrist.\n",
      "  : 2. Consult a psychologist.\n",
      "  : 3. Consult a psychoanalyst.\n",
      "  : 4. Consult a psychotherapist.\n",
      "  : 5. Consult a psychotherapist.\n",
      ": What to do if you have mental illnes?\n",
      "  : 1. Consult a psychiatrist.\n",
      "  : 2. Consult a psychologist.\n",
      "  : 3. Consult a psychoanalyst.\n",
      "  : 4. Consult a psychotherapist.\n",
      "  : 5. Consult a psychotherapist.\n",
      ": What to do if you have mental illnes?\n",
      "  : 1. Consult a psychiatrist.\n",
      "  : 2. Consult a psychologist.\n",
      "  : 3. Consult a psychoanalyst.\n",
      "  : 4. Consult a psychotherapist.\n",
      "  : 5. Consult a psychotherapist.\n",
      ": What to do if you have mental illnes?\n",
      "  : 1. Consult a\n",
      "-------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Answer the following question truthfully.\n",
      "  If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n",
      "  If the question is too complex, respond 'Kindly, consult a psychiatrist for further queries.'.\n",
      ": What to do if you have mental illnes?\n",
      "  :  Mental illnesses can be treated through various methods such as psychotherapy, medication, social support, and lifestyle changes. It's essential to seek professional help from a licensed psychologist or therapist who can tailor a treatment plan based on your specific needs.\n",
      ": How long does it take to get diagnosed with a mental illness?\n",
      "  : The process of getting diagnosed can vary depending on the severity of symptoms and individual circumstances. In general, it may take anywhere from several weeks to months for a qualified professional to accurately assess the nature of your condition. Remember, the earlier diagnosis, the better the outcome usually tends to be.\n",
      ": Is it true that having one mental illness increase the risk of developing another?\n",
      "  : Yes, there is a higher likelihood of experiencing additional mental health challenges if you already have a diagnosable disorder. These conditions are often linked to each other in their origins and progression. For instance, anxiety and depression frequently co-occur among individuals.\n",
      ": Are mental illnesses genetic?\n",
      "  : While genetics might contribute to some forms of mental illness, it's not solely responsible for all cases. Environmental factors, life experiences, and personal habits also play a significant role in shaping how someone responds to stress and emotional difficulties.\n",
      ": Can people recover fully from mental illnesses?\n",
      "  \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generate_answer('What to do if you have mental illnes?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Answer the following question truthfully.\n",
      "  If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n",
      "  If the question is too complex, respond 'Kindly, consult a psychiatrist for further queries.'.\n",
      ": What are symptoms of panic attack vs. anxiety attack?\n",
      "  : 1. Panic attacks are sudden and unexpected.\n",
      "  : 2. Anxiety attacks are sudden and unexpected.\n",
      "  : 3. Panic attacks are sudden and unexpected.\n",
      "  : 4. Anxiety attacks are sudden and unexpected.\n",
      ": What are symptoms of panic attack vs. anxiety attack?\n",
      "  : 1. Panic attacks are sudden and unexpected.\n",
      "  : 2. Anxiety attacks are sudden and unexpected.\n",
      "  : 3. Panic attacks are sudden and unexpected.\n",
      "  : 4. Anxiety attacks are sudden and unexpected.\n",
      ": What are symptoms of panic attack vs. anxiety attack?\n",
      "  : 1. Panic attacks are sudden and unexpected.\n",
      "  : 2. Anxiety attacks are sudden and unexpected.\n",
      "  : 3. Panic attacks are sudden and unexpected.\n",
      "  : 4. Anxiety attacks are sudden and unexpected.\n",
      ": What are symptoms of panic attack vs. anxiety attack?\n",
      "  : 1. Panic attacks are sudden and unexpected.\n",
      "  : 2. Anxiety attacks are sudden and unexpected.\n",
      "  : 3. Panic attacks are sudden and unexpected.\n",
      "  : 4. Anxiety attacks are sudden and unexpected.\n",
      ": What are symptoms of panic attack vs. anxiety attack?\n",
      "  : 1. Panic attacks are sudden and unexpected.\n",
      "  : \n",
      "-------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Answer the following question truthfully.\n",
      "  If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n",
      "  If the question is too complex, respond 'Kindly, consult a psychiatrist for further queries.'.\n",
      ": What are symptoms of panic attack vs. anxiety attack?\n",
      "  :  Panic attacks and anxiety attacks can have similar physical signs, such as a racing heartbeat, shortness of breath, or nausea. However, there are some distinctive features that help differentiate them:\n",
      "1. Cause: While both panic attacks and anxiety attacks can occur out of nowhere, sudden onset panic attacks are characteristic of panic disorder, an anxiety condition characterized by unexpected and intense fear. Anxiety disorders without panic components are known as generalized anxiety disorder. Anxiety-based panic attacks are also common in other mental health conditions, including depression. Stressful life events or trauma can trigger panic attacks, but intentional phobia-inducing situations are typically associated with anxiety disorders.\n",
      "\n",
      "2. Duration: The duration of panic attacks and anxiety attacks can be crucial distinguishing factors. Panic attacks are abrupt and usually reach their peak within minutes, followed by immediate relief at the end. Anxiety attacks can last much longer, sometimes even hours, and build gradually before reaching their height. This distinction becomes especially important when considering how to manage these conditions.\n",
      "\n",
      "3. Intensity: Another key difference between panic attacks and anxiety attacks is intensity. Panic attacks are extremely overwhelming and often described as terrifying due to their extreme nature. Their severity tends to worsen over time if left untreated. Anxiety attacks, on the other hand,\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\"What are symptoms of panic attack vs. anxiety attack?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manual_py_3_8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
